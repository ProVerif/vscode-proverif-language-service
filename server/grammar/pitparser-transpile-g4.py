def get_rule_content(ruleContent):
    # remove control symbols & comments
    cleanedRuleContent = []
    for content in ruleContent:
        normalizedContent = content.strip()
        # remove comments at end of line
        if normalizedContent.find("/*") > 0 and normalizedContent.find("*/") == -1:
            normalizedContent = normalizedContent[0: normalizedContent.rfind("/*")]
        # remove precedence declarations
        if normalizedContent.find("%prec") > 0:
            normalizedContent = normalizedContent[0: normalizedContent.find("%prec")]
        cleanedRuleContent.append(normalizedContent)
    return cleanedRuleContent

def normalize_entries(entries):
    normalized = []
    for entry in entries:
        normalized_entry = entry.strip("|").strip()
        normalized_entry = normalized_entry.replace("  ", " ")
        normalized_entry = normalized_entry.replace("  ", " ")
        normalized_entry = normalized_entry.replace("  ", " ")

        normalized.append(normalized_entry)


    # try to subsume candidate by entry:
    # if entry = lib EQUIVALENCE tprocess tprocess DOT EOF
    # and candidate = lib EQUIVALENCE tprocess tprocess EOF
    # then remove candidate from normalized
    # and change generalized_entry to lib EQUIVALENCE tprocess tprocess ?DOT EOF
    # only do transformation for words in uppercase
    for index, entry in enumerate(normalized):
        for candidate_index, candidate in enumerate(normalized):
            if entry is None or candidate is None or candidate == entry:
                continue

            # split into words (see normalization first loop for why this should work)
            entry_words = entry.split()
            candidate_words = candidate.split()
            if len(candidate_words) >= len(entry_words):
                continue

            # attempt transformation
            candidate_offset = 0
            subsumption_failed = False
            for j in range(len(entry_words)):
                candidate_in_bounds = len(candidate_words) > (j - candidate_offset)
                candidate_word = candidate_words[j - candidate_offset] if candidate_in_bounds else None
                entry_word = entry_words[j].strip('?')
                if entry_word == candidate_word:
                    continue

                if entry_word not in ['SEMI', 'DOT']:
                    subsumption_failed = True
                    break

                entry_words[j] = entry_word + "?"
                candidate_offset += 1

            if subsumption_failed:
                continue

            # prefer to replace rule that comes first
            if candidate_index < index:
                candidate_index, index = index, candidate_index

            normalized[index] = " ".join(entry_words)
            normalized[candidate_index] = None

    return [x for x in normalized if x is not None]


with open('pitparser.mly', 'r') as reader:
    content = reader.read()

    body = content[content.find("%%") + 2:]  # cut off header
    lines = body.splitlines()

    rules = {}
    ruleName = None
    ruleContent = []
    inStarComment = False
    inBracketComment = False
    for index, line in enumerate(lines):
        normalizedLine: str = line.strip()
        if inStarComment or normalizedLine.startswith("/*"):
            inStarComment = True
            if normalizedLine.endswith("*/"):
                inStarComment = False
            continue
        if inBracketComment or normalizedLine.startswith("{"):
            inBracketComment = True
            if normalizedLine.endswith("}"):
                inBracketComment = False

            if ruleName and len(ruleContent) == 0:
                ruleContent.append('')
            continue

        if normalizedLine.endswith(":"):
            if ruleName:
                rules[ruleName] = get_rule_content(ruleContent)
            ruleStarts = True
            ruleName = normalizedLine
            ruleContent = []
        elif ruleName and len(ruleContent) == 0 and not normalizedLine.startswith("/*") and not normalizedLine.startswith("//"):
            ruleContent.append(normalizedLine)
        elif ruleName and normalizedLine.startswith('|'):
            ruleContent.append(normalizedLine)
        elif ruleName and len(ruleContent) > 0 and ruleContent[len(ruleContent)-1].strip() == '|':
            ruleContent[len(ruleContent) - 1] += normalizedLine
        else:
            print("Ignoring line " + str(index+1) + ": " + normalizedLine)

    if ruleName:
        rules[ruleName] = get_rule_content(ruleContent)

    resultLines = [
        '// generated by pitparser-transpile-g4.py',
        '// DANGER: THIS GRAMMAR IS NOT 100% ACCURATELY TRANSPILED. But sufficiently to make this language server work.',
        'parser grammar ProverifParser;',
        'options { tokenVocab = ProverifLexer; }',
        ''
    ]

    # remove rules which we do not need
    rules.pop('onepermut:')
    rules.pop('permut:')
    rules.pop('order:')

    for rule in rules.keys():
        resultLines.append(rule.strip(":"))
        if len(rules[rule]) == 1:
            normalizedEntry = rules[rule][0].strip("|").strip()
            resultLines[len(resultLines) - 1] += ": " + normalizedEntry + ";"
            resultLines.append("")
            continue
        else:
            entries = normalize_entries(rules[rule])
            for index, entry in enumerate(entries):
                if index == 0:
                    resultLines.append("    : " + entry)
                else:
                    resultLines.append("    | " + entry)

            resultLines.append("    ;")
            resultLines.append("")

    result = '\n'.join(resultLines)
    # replace reserved keyword
    result = result.replace(" options", " options_").replace("options_eq", "optionseq").replace("\noptions\n", "\noptions_\n")

    with open('ProverifParser.g4', 'w') as writer:
        writer.write(result)
